% !TEX encoding = UTF-8
% !TEX TS-program = pdflatex
% !TEX root = ../../tesi.tex
\subsubsection{Apache JMeter}
Apache JMeter è stato sviluppato da Apache e presenta la sua prima versione nel 1998. È considerato il leader degli strumenti di test di carico nel mercato open source: La sua longevità vanta una vastissima community attiva e il prodotto offre un esteso parco di funzionalità.\\ 
E’ scritto in Java richiedendo una certa familiarità con la JVM e ha una curva di apprendimento piuttosto ripida: l’interfaccia risulta molto complessa e ci vuole del tempo per prendere familiarità con tutte le funzionalità offerte dal prodotto.\\
Utilizza un \gls{dsl} di XML ed è sconsigliato scrivere i test a mano, rendendo molto difficile la modularità e il versionamento.\\
I test vengono eseguiti via \gls{cli} e sono molto performanti, il software riesce a generare molti utenti concorrenti e supporta l’esecuzione distribuita tramite architettura master/slave sfruttando la tecnologia Java RMI e rendendo possibile la generazione di carichi molto elevati. \\
Permette di generare gli scnari di test registrando la navigazione utente sul browser ed esiste il plugin  ufficiale per Jenkins.\\
Il report generato (in formato HTML) è molto completo ed è possibile esportare i dati grezzi in formato XML o CSV, permettendo di creare documenti in modo personalizzato. \\
È IL prodotto per l’enterprise: maturo, vivo, versatile e performante, oltre ad essere completamente gratuito. L'unica pecca si riscontra nella stesura dei test: l'interfaccia grafica può rendere macchinosa la fase di sviluppo, rendendo molto difficile il loro versionamento e/o la loro modifica al di fuori dell'ambiente integrato.
\subsubsection{Gatling}
Attivo da circa 5 anni, Gatling può essere considerato come una versione più moderna (orientata più verso gli sviluppatori che ai QA) di Jmeter. Entrambi infatti girano sulla JVM (nonostante Gatling sfrutti Scala) e offrono prestazioni molto simili. Tuttavia la completezza e accuratezza dei report di Jmeter sono migliori rispetto a quelli di Gatling.
Gatling offre la possibilità di versionare e modularizzare i test cases tramite un DSL di Scala, pensato per essere usato anche da chi non ha molta competenza con il linguaggio.
Inoltre offre l'opportunità di registrare i vari scenari di test tramite il browser.
Nasce come versione free ma permette l’upgrade alla versione enterprise che offre un pannello di controllo e migliora le metriche per il servizio, oltre a migliorare l’integrazione con il cloud. Ad esempio la versione enterprise offre la possibilità di eseguire i test in modo distribuito tramite il pannello di controllo mentre quella free necessita di configurazioni manuali (gatling permette comunque l’aggregazione dei report in modo da facilitarne l’uso distribuito).
Il report viene fornito in formato HTML utilizzando la libreria highcharts per i grafici (personalizzabili), l’output purtroppo è in plain text ma esistono librerie che ne permettono la conversione in CSV in modo da utilizzarli in servizi esterni.
Prevede un sistema di assertions in modo da validare il risultato di un test ed è presente un plugin ufficiale per l’integrazione con Jenkins.
Anche gatling è un prodotto production/enterprise ready e compensa una leggermente minore completezza rispetto a JMeter con l’avvicinamento alle pratiche DevOps.
\subsubsection{Locust}
Tool di performance testing scritto da sviluppatori per sviluppatori, scritto in python con particolare attenzione all’estensibilità. Punto forte del software è appunto l’estensibilità: la documentazione permette di aggiungere funzionalità a Locust tramite righe di codice in Python. Forse troppo sicuro di questa forza Locust prevede poche funzionalità out of the box supportando solo il protocollo HTTP e offrendo un report molto basilare. L’esecuzione dei test (con relativa configurazione del carico) può avvenire tramite da web-ui o da CLI; la web-ui permette di osservare in real-time le reazioni del server e a lavoro finito offre la possibilità di scaricare il report in formato CSV, mentre l’esecuzione via CLI presenta un report in plain-text in stdout.
I test-cases vengono scritti completamente in python e possono essere quindi modularizzati e versionati ma il prodotto non presenta funzionalità di browser recording rendendo oneroso lo sviluppo di test complessi.
L’utilizzo delle risorse è molto basso ma sulle performance ho dei dubbi: gli articoli che ho utilizzato per i benchmark raccontano due storie completamente diverse: da una parte load impact dice che la potenza di fuoco è molto bassa e le misurazioni poco adatte mentre blazemeter tratta Locust come il più potente dei tool in esame. L’unico modo per avere un responso è provare.
Non esistono plugin ufficiali per jenkins ma è appunto possibile svilupparne uno tramite Python.
Sostanzialmente Locust è un prodotto focalizzato molto per gli sviluppatori e a causa di questo si porta con sé l’onere di sviluppare: con tempo e volontà è sicuramente possibile implementare un buon sistema di report e integrazione continua.
Allo stato attuale Locust è adeguato per testare sistemi poco complessi ma magari, con l’espansione della community, in un futuro sarà possibile utilizzarlo anche per applicazioni enterprise con più sicurezza e meno “smanettamenti”.
UPDATE: pare che effettivamente Locust abbia una potenza di fuoco bassa lanciato su single core e che il modo per avere risultati migliori sia imbastire un buon sistema distribuito.
\subsubsection{The Grinder}
The grinder è un tool di load testing scritto in Java nel 2000. Pare essere un tool completo e performante ma viola il primo requisito della ricerca: l’ultima release ufficiale risale al 2012 mentre l’ultimo commit è del 2015 (la repository si trova su Sourceforge).
Evenutalmente esiste un repository anche su Github che è stato aggiornato di recente ma, osservando le statistiche, è poco diffuso a non ha molti manutentori.
\subsubsection{K6}
K6 è un tool di load testing che si autodefinisce “come dovrebbe essere un tool di load testing nel 2017”.
Piccola nota: molta della letteratura trovata sul tema (un po’ tutti i tool in generale) è fornita da Load Impact, società che ha creato il tool K6, quindi le valutazioni su questo strumento potrebbero essere un po’ di parte. Tuttavia cercando sul web opinioni da parte di altri il tool risulta comunque molto valido. 
Offre un’interfaccia di scripting in javascript, risultando molto valido per il versionamento e la modularizzazione degli script. Il prodotto è developer centric e l’esecuzione dei test via cli unito ad un sistema di assertions/checks lo rende facilmente integrabile in ambienti di CI come Jenkins.
Il reporting avviene sulla CLI ma è minimale e risicato, questo è voluto in quanto il prodotto sembra essere parte di una strategia che invogli l’utente alla sottoscrizione di abbonamenti con Load Impact che offre un più completo sistema di analisi (è presente infatti un comando che permette di eseguire i test direttamente sui server di LoadImpact).
Tuttavia è possibile esportare i dati in formato JSON che contiene informazioni molto più dettagliate rispetto al report standard, inoltre è presente una forte integrazione con InfluxDB a cui è possibile allacciare Grafana (ben documentato sul sito).
Le performance su un singolo computer sono simili a quelle di Gatling ma al momento non è possibile eseguire i test in modalità distribuita (al netto di orchestrazioni personalizzate), tuttavia la feature è nella roadmap per fine anno (anche se non ci sono certezze da questo punto di vista).
\subsubsection{Taurus}
Taurus è un tool per il performance testing pensato con la Continuous Integration in mente, infatti contiene un sistema di asserzioni molto semplice ed esiste il plugin di Jenkins ufficiale dedicato.
Inoltre è stato pensato per risolvere i problemi di automazione/CI di buona parte dei tool di load test presenti sul mercato è offre quindi una vasta libertà di scelta su quale usare. Taurus è capace di eseguire file di JMeter, Gatling, Tsung, Grinder e altri tool di test di carico fino ad arrivare alla simulazione effettiva di un browser tramite Selenium (oltre a supportare tool di functional testing come Robot), lasciando all’utente la possibilità di configurare il tipo di carico da applicare tramite comodi file YAML.
Non sono presenti in giro dei benchmark ma leggendo in giro pare essere un tool molto pesante (è scritto in java e simula altri tool).
Come funzionalità non ha eguali ma essendo sviluppato dal team di Blazemeter per il loro servizio di Cloud non può offrire tutte queste funzionalità gratuitamente: la reportistica infatti è minimale (nonostante quella live sia molto valida nella console) e per ottenere risultati persistenti è necessario sottoscriversi al blazemeter report, che nella versione free permette di di mantenere i log (e visualizzarli tramite il servizio) per 7 giorni.
Inoltre è possibile eseguire direttamente Taurus su blazemeter tramite opportune configurazioni.
Sarebbe il tool perfetto se non fosse a pagamento.
UPDATE: Taurus produce una cartella con tutti i log generati durante l’esecuzione, questi però non sono a formato fisso in quanto dipendono dai metodi utilizzati per il testing (JMeter, Gatling, Selenium, etc.), quindi volendo è possibile generare reportistica ma necessita di un’elaborazione adeguata dei log.
\subsubsection{Vegeta}
Vegeta è un tool che fa della semplicità d’utilizzo e della versatilità dei report il suo punto forte, la semplice CLI permette infatti di imbastire un test in pochi minuti e i risultati generati sono ben dettagliati e offrono diverse possibilità di visualizzazione, a partire dai risultati finali fino ad arrivare ad ogni singola richiesta con il proprio timestamp preciso. I risultati inoltre possono essere esportati direttamente in file HTML che sfruttano Dynochart per la visualizzazione, mentre tramite l’uso di jplot e jaggr è possibile ottenere il report in real-time durante l’esecuzione dei test.
Le performance sono buone (leggermente superiori a quelle di gatling) ed è possibile creare tanti carichi tramite la load distribution (esempio); questa però non è built-in nel sistema e necessita di orchestrazioni esterne per essere funzionante.
Vegeta offre anche la possibilità di scrivere ed eseguire i test tramite una libreria in GO, permettendo di personalizzare l’esecuzione e imbastire un sistema adatto per l’automazione/CI.
Tuttavia la semplicità d’utilizzo di Vegeta giunge ad un compromesso con le poche funzionalità messe a disposizione: non è possibile infatti descrivere test-cases complessi. La definizione dei target infatti, si limita a colpire in round-robin gli url indicati senza poter simulare un vero e proprio comportamento dell’utente.
Inoltre la configurazione di vegeta permette poco controllo sulla distribuzione del carico: il tool infatti permette unicamente di indicare la quantità di richieste per secondo desiderate (caratteristica che in pochi altri tool riescono ad offrire) lasciando libertà al tool di aprire thread/connessioni per raggiungere la potenza di fuoco desiderata. Questa caratteristica non sopperisce però alla mancanza di ramp-up (anche se possibile realizzarla via bash scripting) e alla possibilità di controllare attivamente le risorse del sistema (request latency, utilizzo di thread, etc.).
In definitiva, Vegeta è un ottimo prodotto per testare singoli endpoint in modo controllato e imbastire sistemi di CI più complessi grazie alla versatilità della libreria in go e la facilità d’integrazione con tool di terze parti; tuttavia per scenari più complessi e configurazioni più avanzate guardare altrove.
\subsubsection{Wrk}
Onestamente non c’è troppo da dire su Wrk, partendo dalla documentazione ufficiale praticamente inesistente, le funzionalità offerte da Wrk non offrono tanto spazio di manovra.
E’ il tool più potente per quanto riguarda il throughput grezzo su una singola macchina, è l’unico strumento in grado di utilizzare al 100\% le risorse della CPU.
Questo avviene però solo in particolari casi: server veloce a rispondere e basso payload (eg. body della richiesta) inviato. Alzando il peso del payload inoltre il tool perde accuratezza nella misurazione del response time.
Non offre la possibilità di distribuire l’esecuzione dei test su più macchine e in rete non si trovano argomenti per farlo a mano, suggerendo che questo non è il suo use-case.
La CLI è comoda e permette di integrare scenari più complessi utilizzando il linguaggio di scripting Lua che permette anche di convertire i risultati nel formato di output desiderato.
Il reporting è molto scarno, built-in è offerta solo la visualizzazione in CLI ma le metriche raccolte sono generiche e non permettono un’analisi dettagliata.
Non esiste letteratura per quanto riguarda eventuali integrazioni in CI, suggerendo anche qui che per l’automazione non è il miglior tool, a meno di personalizzazioni in Lua.
\subsubsection{Apachebench}
Come per Wrk, anche Apache Bench (ab) non ha molto di raccontare, sviluppato per testare le performance del webserver apache, svolge un ottimo lavoro nel colpire un singolo URL e riportarne i risultati.
Data la sua maturità e ottimizzazione è in grado di generare un grosso carico sfruttando unicamente un singolo core della CPU ma oltre non va: non supporta l’esecuzione distribuita e non sfrutta CPU multicore.
Genera un carico dimezzato rispetto a Wrk ma offre un report ed un’accuratezza migliore, nonostante non sia possibile esportare il report in formati universali di default, richiedendo uno script/parser ad hoc in caso si voglia esportare i dati dalla CLI.
Tramite la configurazione a linea di comando è possibile specificare il numero di Virtual Users, il numero max di richieste da eseguire e/o la durata massima del test.
Ottimo per test semplici e feedback immediati ma non adeguato per automatizzazione o integrazione in CI.
\subsubsection{Tsung}
%TODO -> citazione a loadimpact: "le funzionalità ci sono, l'usabilità un po' meno"
Tsung è uno strumento di test di carico molto robusto e versatile, è in grado di testare diversi protocolli (dall’HTTP(S) fino a TCP/UDP grezzo) e offre la possibilità di registrare il browser per generare gli scenari di test. Ha un support built in all’esecuzione distribuita dei test moto semplice da realizzare, riuscendo a generare grossi carichi senza particolari difficoltà.
Ha una curva di apprendimento molto meno ripida rispetto a JMeter pur offrendo un sacco di funzionalità e possiede un ottimo modulo di reportistica.
Oltre a tenere traccia di tutte le richieste infatti è in grado di fornire risultati finali elaborati in modo statistico e tramite un’interfaccia web è in grado di mostrare real time l’esecuzione dei test. I log possono essere esportati in HTML e in JSON per essere utilizzabili da un’altra piattaforma.
Insomma offre un sacco di funzionalità interessanti e ben progettate ma si porta con sé due maggiori inconvenienti: l’usabilità e la predisposizione all’integrazione continua.
Gli scenari infatti possono essere registrati via browser ma l’editing e le configurazioni aggiuntive devono essere scritte tramite un DSL di XML che, per quanto possa essere semplice ed intuitivo, non possiede la stessa espressività e flessibilità dei linguaggi di scripting (anche se XML rimane versionabile e modularizzabile).
Per la seconda invece Tsung pare non avere intenzione di navigare verso la CI e sebbene qualcuno sia riuscito con successo ad implementare meccanismi di automazione (esempio), l’integrazione necessita di un lavoro aggiuntivo.
In definitiva Tsung è un ottimo strumento, bisognerebbe capire quanto sia fattibile imbastire un sistema automatico di recupero dei risultati.
\subsubsection{Artillery}
Artillery è un tool di test di carico nato da sviluppatori per sviluppatori, è sviluppato in Javascript ed è orientato verso le infrastrutture di CI proponendo un sistema di pass/fail durante la sua esecuzione.
Può configurare il carico e gli scenari tramite YAML (o JSON) offrendo la possibilità di integrare codice e plugin javascript all’interno dei file grazie ad un sistema di templating.
E’ estensibile tramite plugin (alcuni già presenti come l’integrazione con InfluxDB) e offre la possibilità di testare non solo HTTP ma anche WebSocket e Socket.io, suggerendo una propensione per l’ambiente Javascript da parte degli sviluppatori.
I report prodotti sono relativi all’analisi statistica finale e vengono forniti in formato HTML ma è possibile esportare i report in CSV o integrarsi con DB come Influx grazie ai Plugin.
Le prestazioni non sono ottimali e il tool non riesce a generare un grande carico, non è prevista una modalità di esecuzione distribuita se non ottenendo la versione Pro/Enterprise che grazie all’integrazione con Aws riesce a distribuire l’esecuzione su più macchine migliorandone le performance.
Nella versione 2.0 è prevista l’esecuzione multicore che dovrebbe migliorare sostanzialmente le prestazioni. (fonte)
\subsubsection{Siege}
Siege è un software di test di carico scritto prevalentemente in C che si concentra sul protocollo HTTP.\\
Dopo diverse ricerche ho notato come Siege non offra grosse prestazioni con test complessi e che non riesca a simulare grossi carichi senza perdere in accuratezza di misurazione.\\
Considerando l'obiettivo di \textbf{Maturità} della ricerca, la poca accuratezza nella misurazione, valutata come bug strutturale, ha portato all'abbandono dello studio di questo prodotto.
\subsubsection{Bees with machine guns}
sUna menzione d’onore è assegnata a Bees with machine guns, pur non essendo aggiornato da più di un anno, questo tool permette di generare grossi carichi in maniera distribuita tramite poche righe di \gls{cli}.\\
Sostanzialmente richiede le credenziali di un account AWS e, tramite una piccola configurazione, imbastisce le macchine EC2 richieste (le crea e installa il software necessario), le utilizza per generare il carico richiesto e poi le spegne, tutto in maniera automatica (è possibile spezzare il processo in 3 step, l’orchestrazione, l’esecuzione e lo spegnimento).\\
Non è un software molto sofisticato, ma per test senza grosse pretese o per l'esecuzione di stress test può risultare una scelta molto efficace,  grazie alla facilità con cui si può generare un grosso traffico d'utenti. 
